{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import mediapipe as mp\n",
    "from albumentations import Compose\n",
    "import pandas as pd\n",
    "from model import EfficientNetLandMark \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def extract_index_nparray(nparray):\n",
    "    index = None\n",
    "    for num in nparray[0]:\n",
    "        index = num\n",
    "        break\n",
    "    return index\n",
    "\n",
    "def calculate_delaunay_triangles(rect, points):\n",
    "    \"\"\"\n",
    "    Calculate Delaunay triangles for a set of points.\n",
    "\n",
    "    Args:\n",
    "        rect: A tuple representing the rectangle in which to calculate the Delaunay triangles.\n",
    "        points: A list of tuples representing the points.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, each containing the indices of the 3 points forming a triangle.\n",
    "    \"\"\"\n",
    "    # Create an instance of Subdiv2D\n",
    "    subdiv = cv2.Subdiv2D(rect)\n",
    "\n",
    "    # Insert points into subdiv\n",
    "    for p in points:\n",
    "        subdiv.insert((int(p[0]), int(p[1])))\n",
    "\n",
    "    # Get Delaunay triangulation\n",
    "    triangle_list = subdiv.getTriangleList()\n",
    "    triangle_list = np.array(triangle_list, dtype=np.int32) \n",
    "    \n",
    "    # Find the indices of triangles in the points array\n",
    "    delaunay_tri = []\n",
    "\n",
    "    for t in triangle_list:\n",
    "        pt1, pt2, pt3 = (t[0], t[1]), (t[2], t[3]), (t[4], t[5])\n",
    "\n",
    "        index_pt1 = np.where((points == pt1).all(axis=1))\n",
    "        index_pt1 = extract_index_nparray(index_pt1)\n",
    "\n",
    "        index_pt2 = np.where((points == pt2).all(axis=1))\n",
    "        index_pt2 = extract_index_nparray(index_pt2)\n",
    "\n",
    "        index_pt3 = np.where((points == pt3).all(axis=1))\n",
    "        index_pt3 = extract_index_nparray(index_pt3)\n",
    "\n",
    "        if index_pt1 is not None and index_pt2 is not None and index_pt3 is not None:\n",
    "            triangle = [index_pt1, index_pt2, index_pt3]\n",
    "            delaunay_tri.append(triangle)\n",
    "\n",
    "    return delaunay_tri\n",
    "\n",
    "def apply_affine_transform(src,src_tri,dst_tri,size):\n",
    "    \"\"\"\n",
    "    Apply affine transform calculated using srcTri and dstTri to src and output an image of size.\n",
    "\n",
    "    Args:\n",
    "      src: Source image.\n",
    "      src_tri: List of tuples representing the vertices of the source triangle.\n",
    "      dst_tri: List of tuples representing the vertices of the destination triangle.\n",
    "      size: A tuple representing the size of the output image.\n",
    "\n",
    "    Returns:\n",
    "      The transformed image and the warp matrix.\n",
    "    \"\"\"\n",
    "    warp_mat = cv2.getAffineTransform(np.float32(src_tri), np.float32(dst_tri))\n",
    "    dst = cv2.warpAffine(\n",
    "        src, warp_mat, (size[0], size[1]), None, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101\n",
    "    )\n",
    "\n",
    "    return dst\n",
    "\n",
    "# Warps and alpha blends triangular regions from img1 and img2 to img\n",
    "def warpTriangle(img1, img2, t1, t2):\n",
    "  # Find bounding rectangle for each triangle\n",
    "  r1 = cv2.boundingRect(t1)\n",
    "  r2 = cv2.boundingRect(t2)\n",
    "  # Offset points by left top corner of the respective rectangles\n",
    "  t1Rect = []\n",
    "  t2Rect = []\n",
    "  t2RectInt = []\n",
    "\n",
    "  for i in range(0, 3):\n",
    "    t1Rect.append(((t1[i][0] - r1[0]), (t1[i][1] - r1[1])))\n",
    "    t2Rect.append(((t2[i][0] - r2[0]), (t2[i][1] - r2[1])))\n",
    "    t2RectInt.append(((t2[i][0] - r2[0]), (t2[i][1] - r2[1])))\n",
    "\n",
    "  # Get mask by filling triangle\n",
    "  mask = np.zeros((r2[3], r2[2], 3), dtype=np.uint8)\n",
    "  cv2.fillConvexPoly(mask, np.int32(t2RectInt), (1, 1, 1), 16, 0)\n",
    "\n",
    "  # Apply warpImage to small rectangular patches\n",
    "  img1Rect = img1[r1[1]:r1[1] + r1[3], r1[0]:r1[0] + r1[2]]\n",
    "\n",
    "  size = (r2[2], r2[3])\n",
    "\n",
    "  img2Rect = apply_affine_transform(img1Rect, t1Rect, t2Rect, size)\n",
    "\n",
    "  img2Rect = img2Rect * mask\n",
    "\n",
    "  # Copy triangular region of the rectangular patch to the output image\n",
    "  try:\n",
    "    img2[r2[1]:r2[1]+r2[3], r2[0]:r2[0]+r2[2]] *= ((1, 1, 1) - mask).astype(np.uint32)\n",
    "    img2[r2[1]:r2[1]+r2[3], r2[0]:r2[0]+r2[2]] += img2Rect\n",
    "  except:\n",
    "      pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def load_landmarks_filter(annotation_file):\n",
    "        df = pd.read_csv(annotation_file, header=None)\n",
    "        x = df[1].values\n",
    "        y = df[2].values\n",
    "        return np.array([(x1, y1) for x1, y1 in zip(x, y)])\n",
    "\n",
    "def load_filter(filter):\n",
    "    filter_path_img = '../filter/' + filter + '.png'\n",
    "    filter_path_csv = '../filter/' + filter + '-annotated.csv'\n",
    "    if os.path.isfile(filter_path_csv):\n",
    "        pass\n",
    "    else:\n",
    "        filter_path_csv = '../filter/' + filter + '.csv'\n",
    "    filter_ìmg = cv2.imread(filter_path_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    filter_landmarks = load_landmarks_filter(filter_path_csv)\n",
    "\n",
    "    points = np.array(filter_landmarks, np.int32)\n",
    "    convexhull = cv2.convexHull(points)\n",
    "    rect = cv2.boundingRect(convexhull)\n",
    "    delau_tri_filter = calculate_delaunay_triangles(rect, points)\n",
    "    \n",
    "    return filter_ìmg, filter_landmarks, delau_tri_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(model, image):\n",
    "    transform = Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    input = np.array(image)\n",
    "    image = Image.fromarray(image)\n",
    "    input = cv2.cvtColor(input, cv2.COLOR_BGR2RGB)\n",
    "    face_detector = mp.solutions.face_detection.FaceDetection(min_detection_confidence=0.5, model_selection=1)\n",
    "    faces = face_detector.process(input)\n",
    "    height, width, _ = input.shape\n",
    "    keypoints = []\n",
    "    # plt.imshow(input)\n",
    "    if not faces.detections:\n",
    "        return faces, keypoints\n",
    "    for face in faces.detections:\n",
    "        box = face.location_data.relative_bounding_box\n",
    "        x_min, y_min, x_max, y_max = box.xmin*width, box.ymin*height, (box.xmin + box.width)*width, (box.ymin + box.height)*height\n",
    "        \n",
    "        cropped_image = np.array(image.crop((x_min, y_min, x_max, y_max)))\n",
    "\n",
    "        height_crop, width_crop, _ = cropped_image.shape\n",
    "\n",
    "        detransform = Compose([\n",
    "            A.Resize(width_crop, height_crop),\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "\n",
    "        transformed = transform(image=cropped_image)\n",
    "\n",
    "        cropped_image = transformed[\"image\"]\n",
    "        \n",
    "        input = cropped_image[None].to(device)\n",
    "\n",
    "        output = model(input)\n",
    "        points = torch.Tensor(output.cpu().reshape((68, 2))).to(torch.float32).detach().numpy()\n",
    "        points = (points + 0.5) * np.array([224, 224])\n",
    "\n",
    "        detransformed = detransform(image=np.zeros((224, 224)), keypoints=points)\n",
    "        points = detransformed[\"keypoints\"]\n",
    "        points = points + np.array([x_min, y_min])\n",
    "        plt.scatter(points[:, 0], points[:, 1], s=3, c='cyan')\n",
    "        keypoints.append(points)\n",
    "\n",
    "    return faces, keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def apply_filter_image (model, image,filter_ìmg, filter_landmarks, delau_tri_filter):\n",
    "\n",
    "    faces, keypoints = get_keypoints(model, image)\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    if not faces.detections:\n",
    "        return image\n",
    "    for i in range (len(faces.detections)):\n",
    "        face_landmarks = keypoints[i]\n",
    "\n",
    "        for tri in delau_tri_filter:\n",
    "            tr1_pt1 = filter_landmarks[tri[0]]\n",
    "            tr1_pt2 = filter_landmarks[tri[1]]\n",
    "            tr1_pt3 = filter_landmarks[tri[2]]\n",
    "            filter_tri = np.array([tr1_pt1, tr1_pt2, tr1_pt3], np.int32)\n",
    "            \n",
    "            if tri[0] <= 67 and tri[1] <= 67 and tri[2] <= 67:\n",
    "                tr2_pt1 = face_landmarks[tri[0]]\n",
    "                tr2_pt2 = face_landmarks[tri[1]]\n",
    "                tr2_pt3 = face_landmarks[tri[2]]\n",
    "                face_tri = np.array([tr2_pt1, tr2_pt2, tr2_pt3], np.int32)\n",
    "                warpTriangle(filter_ìmg, image, filter_tri, face_tri)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    return image\n",
    "        \n",
    "\n",
    "def apply_filter_video (model, filter):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_FPS, 15)\n",
    "\n",
    "    filter_ìmg, filter_landmarks, delau_tri_filter = load_filter(filter)\n",
    "    while cap.isOpened():\n",
    "        _, image = cap.read()\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = apply_filter_image(model, image, filter_ìmg, filter_landmarks, delau_tri_filter)\n",
    "        image = cv2.flip(image, 1)\n",
    "        cv2.imshow(\"Result\", image)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "numOfPoints = 68\n",
    "model = EfficientNetLandMark(numOfPoints)\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=torch.device('cpu')))\n",
    "device = torch.device('cpu')\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "image_path = '../300w/ibug/image_005_1_mirror.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "apply_filter_video(model,'anonymous')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
